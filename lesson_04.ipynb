{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give a model time to think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our upcoming strategy involves guiding the model to independently devise solutions before hastily reaching a conclusion. Occasionally, we achieve superior outcomes by explicitly directing the models to engage in reasoning and formulating their solutions before drawing conclusions.\n",
    "\n",
    "I would like to emphasize the importance of allowing the model sufficient time to actively process information before determining the correctness of an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üéØ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-Yuu6ZbvcrmJ6aYp5IUQiT3BlbkFJMcylU\n",
      "First LLM API example\n",
      "‚úÖ OpenAI Key loaded (sk-Yuu6ZbvcrmJ6aYp5IUQiT3BlbkFJMcylU...)\n"
     ]
    }
   ],
   "source": [
    "from util import local_settings\n",
    "from env_colors import TerminalTextColor as ttc\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"First LLM API example\")\n",
    "print(f\"‚úÖ OpenAI Key loaded ({local_settings.OPENAI_API_KEY[0:-15]}...)\")\n",
    "\n",
    "client = OpenAI(api_key=local_settings.OPENAI_API_KEY)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0, messages=None):\n",
    "    if not messages:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tactic 1:\n",
    "\n",
    "#### Specify the steps required to complete a task\n",
    "\n",
    "If a model is making reasoning errors by  rushing to an incorrect conclusion, you should try reframing the query to request a chain or series of relevant reasoning before the model provides its final answer.\n",
    "\n",
    "Another way to think about this is that if you give a model a task that's too complex for it to do in a short amount of time or in a small number of words, it may make up a guess which is likely to be incorrect.\n",
    "\n",
    "So, in these situations, you can instruct the model to think longer about a problem, which means it's spending more computational effort on  the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for prompt 1:\n",
      "1 - The text tells the story of siblings Jack and Jill who go on a quest to fetch water from a hilltop well, but encounter misfortune when Jack trips and tumbles down the hill, with Jill following suit. Despite the mishap, they return home and continue exploring with delight.\n",
      "\n",
      "2 - El texto cuenta la historia de los hermanos Jack y Jill que van en una b√∫squeda para traer agua de un pozo en la cima de una colina, pero encuentran desgracia cuando Jack tropieza y cae rodando por la colina, seguido por Jill. A pesar del percance, regresan a casa y contin√∫an explorando con alegr√≠a.\n",
      "\n",
      "3 - Jack, Jill\n",
      "\n",
      "4 - {\n",
      "  \"spanish_summary\": \"El texto cuenta la historia de los hermanos Jack y Jill que van en una b√∫squeda para traer agua de un pozo en la cima de una colina, pero encuentran desgracia cuando Jack tropieza y cae rodando por la colina, seguido por Jill. A pesar del percance, regresan a casa y contin√∫an explorando con alegr√≠a.\",\n",
      "  \"names\": [\"Jack\", \"Jill\"],\n",
      "  \"num_names\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on\n",
    "a quest to fetch water from a hilltop\n",
    "well. As they climbed, singing joyfully, misfortune\n",
    "struck‚ÄîJack tripped on a stone and tumbled\n",
    "down the hill, with Jill following suit.\n",
    "Though slightly battered, the pair returned home to\n",
    "comforting embraces. Despite the mishap,\n",
    "their adventurous spirits remained undimmed, and they\n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions:\n",
    "1 - Summarize the following text delimited by triple\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into Spanish.\n",
    "3 - List each name in the Spanish summary.\n",
    "4 - As the final topic of the output, generate a JSON object with the names in the Spanish summary that contains the following keys: spanish_summary, names, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt_1)\n",
    "\n",
    "print(\"Completion for prompt 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask for a specific output\n",
    "\n",
    "You can specify the desired result after outlining the task steps to guide a particular output and ensure its expected occurrence. In this scenario, the model had the opportunity to methodically organize its reasoning by executing the task step by step and fulfilling the output accurately.\n",
    "\n",
    "`Task`\n",
    "\n",
    "`Steps`\n",
    "\n",
    "`Output Format`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completion for prompt 2:\n",
      "Summary: Jack and Jill go on a quest to fetch water from a hilltop well, but encounter misfortune when Jack trips and tumbles down the hill, with Jill following suit, yet they remain undeterred and continue exploring.\n",
      "\n",
      "Translation: Jack et Jill partent √† la recherche d'eau d'un puits au sommet d'une colline, mais rencontrent un malheur lorsque Jack tr√©buche et d√©vale la colline, suivi de Jill, mais ils restent d√©termin√©s et continuent √† explorer.\n",
      "\n",
      "Names: Jack, Jill\n",
      "\n",
      "Output JSON: {\"french_summary\": \"Jack et Jill partent √† la recherche d'eau d'un puits au sommet d'une colline, mais rencontrent un malheur lorsque Jack tr√©buche et d√©vale la colline, suivi de Jill, mais ils restent d√©termin√©s et continuent √† explorer.\", \"num_names\": 2}\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Your task is to perform the following actions:\n",
    "1 - Summarize the following text delimited by\n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the\n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "\n",
    "Summary: <summary>\n",
    "\n",
    "Translation: <summary translation>\n",
    "\n",
    "Names: <list of names in Italian summary>\n",
    "\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt_2)\n",
    "\n",
    "print(\"\\nCompletion for prompt 2:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tactic 2: \n",
    "\n",
    "#### Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we get better results when explicitly instructing the models to explain their solutions before concluding. \n",
    "\n",
    "This is the same idea we were discussing, giving the model time to work things out before just saying if an answer is correct, in the same way a person would. Think first :)\n",
    "\n",
    "So, in this prompt, we're asking the model to determine if the student's solution is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's solution is correct. They correctly identified the costs for land, solar panels, and maintenance, and calculated the total cost as a function of the number of square feet.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financial.\n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations\n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùå Note that the student's solution is actually not correct.\n",
    "\n",
    "We can fix this by instructing the model to work out its own solution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate the total cost for the first year of operations, we need to add up the costs of land, solar panels, and maintenance.\n",
      "\n",
      "Let x be the size of the installation in square feet.\n",
      "\n",
      "1. Land cost: $100 / square foot\n",
      "The cost of land is calculated by multiplying the size of the installation (x) by the cost per square foot ($100):\n",
      "Land cost = 100 * x\n",
      "\n",
      "2. Solar panel cost: $250 / square foot\n",
      "The cost of solar panels is calculated by multiplying the size of the installation (x) by the cost per square foot ($250):\n",
      "Solar panel cost = 250 * x\n",
      "\n",
      "3. Maintenance cost: $100,000 + $10 / square foot\n",
      "The maintenance cost is a flat fee of $100,000 per year, plus an additional $10 per square foot. So the maintenance cost is calculated by adding the flat fee to the product of the size of the installation (x) and the cost per square foot ($10):\n",
      "Maintenance cost = 100,000 + 10 * x\n",
      "\n",
      "Total cost for the first year of operations:\n",
      "Total cost = Land cost + Solar panel cost + Maintenance cost\n",
      "Total cost = 100 * x + 250 * x + 100,000 + 10 * x\n",
      "Total cost = 360 * x + 100,000\n",
      "\n",
      "Is the student's solution the same as the actual solution just calculated:\n",
      "No\n",
      "\n",
      "Student grade:\n",
      "Incorrect\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem.\n",
    "- Then compare your solution to the student's solution\n",
    "and evaluate if the student's solution is correct or not.\n",
    "Don't decide if the student's solution is correct until\n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financial.\n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
